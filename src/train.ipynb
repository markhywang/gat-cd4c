{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.13.2' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/sbin/python -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "# Local imports\n",
    "from model import GraphAttentionNetwork\n",
    "from utils.dataset import DrugProteinDataset\n",
    "from utils.helper_functions import set_seeds, accuracy_func, plot_loss_curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(args) -> None:\n",
    "    set_seeds()\n",
    "\n",
    "    model = GraphAttentionNetwork(\n",
    "        333, 1, 16,\n",
    "        args['hidden_size'],\n",
    "        args['num_layers'],\n",
    "        args['num_attn_heads']\n",
    "    ).to(device)\n",
    "\n",
    "    train_dataset, validation_dataset, test_dataset = load_data(\n",
    "        args['data_path'], args['seed'],\n",
    "        args['frac_train'], args['frac_validation'], args['frac_test'],\n",
    "        args['use_small_dataset']\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=args['batch_size'], shuffle=True)\n",
    "    validation_loader = DataLoader(validation_dataset, batch_size=args['batch_size'], shuffle=False)\n",
    "\n",
    "    loss_func = nn.SmoothL1Loss(beta=args['huber_beta'])\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])\n",
    "    lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=args['scheduler_factor'], patience=args['scheduler_patience'])\n",
    "\n",
    "    best_validation_loss = float('inf')\n",
    "    no_validation_loss_improvement = 0\n",
    "    metrics_df = pd.DataFrame(columns=['train_loss', 'validation_loss'], index=range(args['max_epochs']))\n",
    "\n",
    "    for epoch in range(args['max_epochs']):\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{args['max_epochs']}\", leave=True)\n",
    "        avg_train_loss, avg_train_acc = run_training_epoch(progress_bar, optimizer, model, loss_func)\n",
    "        avg_validation_loss, avg_validation_acc = get_validation_metrics(validation_loader, model, loss_func)\n",
    "\n",
    "        lr_scheduler.step(avg_train_loss)\n",
    "\n",
    "        metrics_df.at[epoch, 'train_loss'] = avg_train_loss\n",
    "        metrics_df.at[epoch, 'validation_loss'] = avg_validation_loss\n",
    "        metrics_df.at[epoch, 'train_acc'] = avg_train_acc\n",
    "        metrics_df.at[epoch, 'validation_acc'] = avg_validation_acc\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}: Train Loss = {avg_train_loss:.5f}, Validation Loss = {avg_validation_loss:.5f}, \"\n",
    "              f\"Train Acc = {avg_train_acc:.5f}, Validation Acc = {avg_validation_acc:.5f}\")\n",
    "\n",
    "        if avg_validation_loss < best_validation_loss:\n",
    "            best_validation_loss = avg_validation_loss\n",
    "            no_validation_loss_improvement = 0\n",
    "            torch.save(model.state_dict(), '../models/model.pth')\n",
    "        else:\n",
    "            no_validation_loss_improvement += 1\n",
    "            if no_validation_loss_improvement == args['stoppage_epochs']:\n",
    "                break\n",
    "\n",
    "    plot_loss_curves(metrics_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training_epoch(progress_bar, optimizer, model, loss_func):\n",
    "    model.train()\n",
    "    cum_samples = cum_loss = cum_acc = 0\n",
    "\n",
    "    for batch_data in progress_bar:\n",
    "        node_features, edge_features, adjacency_matrix, pchembl_score = [\n",
    "            x.to(torch.float32).to(device) for x in batch_data\n",
    "        ]\n",
    "\n",
    "        preds = model(node_features, edge_features, adjacency_matrix).squeeze(-1)\n",
    "        loss = loss_func(preds, pchembl_score)\n",
    "\n",
    "        cum_samples += preds.shape[0]\n",
    "        cum_loss += loss.item() * preds.shape[0]\n",
    "        cum_acc += accuracy_func(preds, pchembl_score, threshold=7.0)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return cum_loss / cum_samples, cum_acc / cum_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_validation_metrics(loader, model, loss_func):\n",
    "    model.eval()\n",
    "    cum_samples = cum_loss = cum_acc = 0\n",
    "\n",
    "    for batch in loader:\n",
    "        node_features, edge_features, adjacency_matrix, pchembl_scores = [\n",
    "            x.to(torch.float32).to(device) for x in batch\n",
    "        ]\n",
    "\n",
    "        preds = model(node_features, edge_features, adjacency_matrix).squeeze(-1)\n",
    "        loss = loss_func(preds, pchembl_scores).item()\n",
    "        acc = accuracy_func(preds, pchembl_scores, threshold=7.0)\n",
    "\n",
    "        cum_samples += preds.shape[0]\n",
    "        cum_loss += loss * preds.shape[0]\n",
    "        cum_acc += acc\n",
    "\n",
    "    return cum_loss / cum_samples, cum_acc / cum_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path, seed, frac_train, frac_val, frac_test, use_small):\n",
    "    assert math.isclose(frac_train + frac_val + frac_test, 1), \"Splits must sum to 1\"\n",
    "\n",
    "    file_name = 'filtered_cancer_small.csv' if use_small else 'filtered_cancer_all.csv'\n",
    "    data_df = pd.read_csv(f'{data_path}/{file_name}')\n",
    "    protein_df = pd.read_csv(f'{data_path}/protein_embeddings.csv', index_col=0)\n",
    "\n",
    "    data_df['stratify_col'] = data_df['Target_ID'] + '_' + data_df['label'].astype(str)\n",
    "\n",
    "    train_df, remain_df = train_test_split(data_df, test_size=frac_val + frac_test,\n",
    "                                           stratify=data_df['stratify_col'], random_state=seed)\n",
    "    val_df, test_df = train_test_split(remain_df, test_size=frac_test / (frac_val + frac_test),\n",
    "                                       stratify=remain_df['stratify_col'], random_state=seed)\n",
    "\n",
    "    train_df = train_df.drop(columns='stratify_col')\n",
    "    val_df = val_df.drop(columns='stratify_col')\n",
    "    test_df = test_df.drop(columns='stratify_col')\n",
    "\n",
    "    train_set = DrugProteinDataset(train_df, protein_df)\n",
    "    val_set = DrugProteinDataset(val_df, protein_df)\n",
    "    test_set = DrugProteinDataset(test_df, protein_df)\n",
    "\n",
    "    return train_set, val_set, test_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"use_small_dataset\": True,\n",
    "    \"batch_size\": 64,\n",
    "    \"stoppage_epochs\": 5,\n",
    "    \"max_epochs\": 128,\n",
    "    \"seed\": 42,\n",
    "    \"data_path\": \"../data\",\n",
    "    \"frac_train\": 0.7,\n",
    "    \"frac_validation\": 0.15,\n",
    "    \"frac_test\": 0.15,\n",
    "    \"huber_beta\": 1.0,\n",
    "    \"weight_decay\": 1e-4,\n",
    "    \"lr\": 3e-3,\n",
    "    \"scheduler_patience\": 10,\n",
    "    \"scheduler_factor\": 0.5,\n",
    "    \"hidden_size\": 32,\n",
    "    \"num_layers\": 4,\n",
    "    \"num_attn_heads\": 8,\n",
    "    \"dropout\": 0.2,\n",
    "    \"leaky_relu_slope\": 0.2\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(args)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/local3/2024/aguo/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Local imports\n",
    "from train import train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"use_small_dataset\": True,\n",
    "    \"batch_size\": 64,\n",
    "    \"stoppage_epochs\": 64,\n",
    "    \"max_epochs\": 512,\n",
    "    \"seed\": 0,\n",
    "    \"data_path\": \"../data\",\n",
    "    \"protein_graph_dir\": \"../data/protein_graphs\",\n",
    "    \"frac_train\": 0.8,\n",
    "    \"frac_validation\": 0.1,\n",
    "    \"frac_test\": 0.1,\n",
    "    \"huber_beta\": 0.5,\n",
    "    \"weight_decay\": 1e-3,\n",
    "    \"lr\": 1e-4,\n",
    "    \"scheduler_patience\": 12,\n",
    "    \"scheduler_factor\": 0.5,\n",
    "    \"hidden_size\": 96,\n",
    "    \"emb_size\": 96,\n",
    "    \"num_layers\": 4,\n",
    "    \"num_attn_heads\": 8,\n",
    "    \"dropout\": 0.0,\n",
    "    \"pooling_dim\": 96,\n",
    "    \"mlp_hidden\": 96,\n",
    "    \"max_nodes\": 64, # Max number of amino acids\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 896845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/512: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 63/63 [00:33<00:00,  1.86it/s]\n",
      "Process SpawnProcess-4:\n",
      "Process SpawnProcess-12:\n",
      "Process SpawnProcess-11:\n",
      "Process SpawnProcess-14:\n",
      "Process SpawnProcess-1:\n",
      "Process SpawnProcess-9:\n",
      "Process SpawnProcess-10:\n",
      "Process SpawnProcess-8:\n",
      "Process SpawnProcess-13:\n",
      "Process SpawnProcess-15:\n",
      "Process SpawnProcess-3:\n",
      "Process SpawnProcess-2:\n",
      "Process SpawnProcess-5:\n",
      "Process SpawnProcess-6:\n",
      "Process SpawnProcess-7:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/local3/2024/aguo/miniconda3/lib/python3.12/multiprocessing/process.py\", line 317, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/scratch/local3/2024/aguo/miniconda3/lib/python3.12/multiprocessing/process.py\", line 317, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/scratch/local3/2024/aguo/miniconda3/lib/python3.12/multiprocessing/util.py\", line 323, in _exit_function\n",
      "    def _exit_function(info=info, debug=debug, _run_finalizers=_run_finalizers,\n",
      "    \n",
      "  File \"/scratch/local3/2024/aguo/miniconda3/lib/python3.12/multiprocessing/util.py\", line 323, in _exit_function\n",
      "    def _exit_function(info=info, debug=debug, _run_finalizers=_run_finalizers,\n",
      "    \n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/scratch/local3/2024/aguo/miniconda3/lib/python3.12/multiprocessing/process.py\", line 317, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/scratch/local3/2024/aguo/miniconda3/lib/python3.12/multiprocessing/util.py\", line 323, in _exit_function\n",
      "    def _exit_function(info=info, debug=debug, _run_finalizers=_run_finalizers,\n",
      "    \n",
      "KeyboardInterrupt\n",
      "  File \"/scratch/local3/2024/aguo/miniconda3/lib/python3.12/multiprocessing/process.py\", line 317, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/scratch/local3/2024/aguo/miniconda3/lib/python3.12/multiprocessing/util.py\", line 323, in _exit_function\n",
      "    def _exit_function(info=info, debug=debug, _run_finalizers=_run_finalizers,\n",
      "    \n",
      "  File \"/scratch/local3/2024/aguo/miniconda3/lib/python3.12/multiprocessing/process.py\", line 317, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/scratch/local3/2024/aguo/miniconda3/lib/python3.12/multiprocessing/process.py\", line 317, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/scratch/local3/2024/aguo/miniconda3/lib/python3.12/multiprocessing/process.py\", line 317, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/scratch/local3/2024/aguo/miniconda3/lib/python3.12/multiprocessing/process.py\", line 317, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/scratch/local3/2024/aguo/miniconda3/lib/python3.12/multiprocessing/process.py\", line 317, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/scratch/local3/2024/aguo/miniconda3/lib/python3.12/multiprocessing/util.py\", line 323, in _exit_function\n",
      "    def _exit_function(info=info, debug=debug, _run_finalizers=_run_finalizers,\n",
      "    \n",
      "  File \"/scratch/local3/2024/aguo/miniconda3/lib/python3.12/multiprocessing/util.py\", line 323, in _exit_function\n",
      "    def _exit_function(info=info, debug=debug, _run_finalizers=_run_finalizers,\n",
      "    \n",
      "KeyboardInterrupt\n",
      "  File \"/scratch/local3/2024/aguo/miniconda3/lib/python3.12/multiprocessing/process.py\", line 317, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/scratch/local3/2024/aguo/miniconda3/lib/python3.12/multiprocessing/util.py\", line 323, in _exit_function\n",
      "    def _exit_function(info=info, debug=debug, _run_finalizers=_run_finalizers,\n",
      "    \n",
      "  File \"/scratch/local3/2024/aguo/miniconda3/lib/python3.12/multiprocessing/util.py\", line 323, in _exit_function\n",
      "    def _exit_function(info=info, debug=debug, _run_finalizers=_run_finalizers,\n",
      "    \n",
      "  File \"/scratch/local3/2024/aguo/miniconda3/lib/python3.12/multiprocessing/util.py\", line 323, in _exit_function\n",
      "    def _exit_function(info=info, debug=debug, _run_finalizers=_run_finalizers,\n",
      "    \n",
      "  File \"/scratch/local3/2024/aguo/miniconda3/lib/python3.12/multiprocessing/util.py\", line 323, in _exit_function\n",
      "    def _exit_function(info=info, debug=debug, _run_finalizers=_run_finalizers,\n",
      "    \n",
      "  File \"/scratch/local3/2024/aguo/miniconda3/lib/python3.12/multiprocessing/process.py\", line 317, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/scratch/local3/2024/aguo/miniconda3/lib/python3.12/multiprocessing/process.py\", line 317, in _bootstrap\n",
      "    util._exit_function()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/scratch/local3/2024/aguo/miniconda3/lib/python3.12/multiprocessing/process.py\", line 317, in _bootstrap\n",
      "    util._exit_function()\n",
      "KeyboardInterrupt\n",
      "  File \"/scratch/local3/2024/aguo/miniconda3/lib/python3.12/multiprocessing/util.py\", line 323, in _exit_function\n",
      "    def _exit_function(info=info, debug=debug, _run_finalizers=_run_finalizers,\n",
      "    \n",
      "  File \"/scratch/local3/2024/aguo/miniconda3/lib/python3.12/multiprocessing/util.py\", line 323, in _exit_function\n",
      "    def _exit_function(info=info, debug=debug, _run_finalizers=_run_finalizers,\n",
      "    \n",
      "  File \"/scratch/local3/2024/aguo/miniconda3/lib/python3.12/multiprocessing/util.py\", line 323, in _exit_function\n",
      "    def _exit_function(info=info, debug=debug, _run_finalizers=_run_finalizers,\n",
      "    \n",
      "  File \"/scratch/local3/2024/aguo/miniconda3/lib/python3.12/multiprocessing/process.py\", line 317, in _bootstrap\n",
      "    util._exit_function()\n",
      "KeyboardInterrupt\n",
      "  File \"/scratch/local3/2024/aguo/miniconda3/lib/python3.12/multiprocessing/util.py\", line 323, in _exit_function\n",
      "    def _exit_function(info=info, debug=debug, _run_finalizers=_run_finalizers,\n",
      "    \n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/scratch/local3/2024/aguo/miniconda3/lib/python3.12/multiprocessing/process.py\", line 317, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/scratch/local3/2024/aguo/miniconda3/lib/python3.12/multiprocessing/util.py\", line 323, in _exit_function\n",
      "    def _exit_function(info=info, debug=debug, _run_finalizers=_run_finalizers,\n",
      "    \n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/scratch/local3/2024/aguo/miniconda3/lib/python3.12/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/scratch/local3/2024/aguo/miniconda3/lib/python3.12/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aguo/gat-cd4c/src/train.py\", line 5, in <module>\n",
      "    from sklearn.model_selection import train_test_split\n",
      "  File \"/scratch/local3/2024/aguo/miniconda3/lib/python3.12/site-packages/sklearn/__init__.py\", line 73, in <module>\n",
      "    from .base import clone  # noqa: E402\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/scratch/local3/2024/aguo/miniconda3/lib/python3.12/site-packages/sklearn/base.py\", line 19, in <module>\n",
      "    from .utils._estimator_html_repr import _HTMLDocumentationLinkMixin, estimator_html_repr\n",
      "  File \"/scratch/local3/2024/aguo/miniconda3/lib/python3.12/site-packages/sklearn/utils/__init__.py\", line 15, in <module>\n",
      "    from ._chunking import gen_batches, gen_even_slices\n",
      "  File \"/scratch/local3/2024/aguo/miniconda3/lib/python3.12/site-packages/sklearn/utils/_chunking.py\", line 11, in <module>\n",
      "    from ._param_validation import Interval, validate_params\n",
      "  File \"/scratch/local3/2024/aguo/miniconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 14, in <module>\n",
      "    from scipy.sparse import csr_matrix, issparse\n",
      "  File \"/scratch/local3/2024/aguo/miniconda3/lib/python3.12/site-packages/scipy/sparse/__init__.py\", line 315, in <module>\n",
      "    from . import csgraph\n",
      "  File \"/scratch/local3/2024/aguo/miniconda3/lib/python3.12/site-packages/scipy/sparse/csgraph/__init__.py\", line 187, in <module>\n",
      "    from ._laplacian import laplacian\n",
      "  File \"/scratch/local3/2024/aguo/miniconda3/lib/python3.12/site-packages/scipy/sparse/csgraph/_laplacian.py\", line 7, in <module>\n",
      "    from scipy.sparse.linalg import LinearOperator\n",
      "  File \"/scratch/local3/2024/aguo/miniconda3/lib/python3.12/site-packages/scipy/sparse/linalg/__init__.py\", line 131, in <module>\n",
      "    from ._isolve import *\n",
      "  File \"/scratch/local3/2024/aguo/miniconda3/lib/python3.12/site-packages/scipy/sparse/linalg/_isolve/__init__.py\", line 4, in <module>\n",
      "    from .iterative import *\n",
      "  File \"/scratch/local3/2024/aguo/miniconda3/lib/python3.12/site-packages/scipy/sparse/linalg/_isolve/iterative.py\", line 5, in <module>\n",
      "    from scipy.linalg import get_lapack_funcs\n",
      "  File \"/scratch/local3/2024/aguo/miniconda3/lib/python3.12/site-packages/scipy/linalg/__init__.py\", line 203, in <module>\n",
      "    from ._misc import *\n",
      "  File \"/scratch/local3/2024/aguo/miniconda3/lib/python3.12/site-packages/scipy/linalg/_misc.py\", line 4, in <module>\n",
      "    from .lapack import get_lapack_funcs\n",
      "  File \"/scratch/local3/2024/aguo/miniconda3/lib/python3.12/site-packages/scipy/linalg/lapack.py\", line 850, in <module>\n",
      "    from scipy.linalg import _flapack\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01margparse\u001b[39;00m\n\u001b[1;32m      3\u001b[0m training_args \u001b[38;5;241m=\u001b[39m argparse\u001b[38;5;241m.\u001b[39mNamespace(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m----> 4\u001b[0m train_model(training_args, device)\n",
      "File \u001b[0;32m~/gat-cd4c/src/train.py:204\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(args, m_device)\u001b[0m\n\u001b[1;32m    201\u001b[0m tma \u001b[38;5;241m=\u001b[39m total[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmae\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m/\u001b[39msamples\n\u001b[1;32m    203\u001b[0m \u001b[38;5;66;03m# validation\u001b[39;00m\n\u001b[0;32m--> 204\u001b[0m v_loss,v_acc,v_mse,v_mae \u001b[38;5;241m=\u001b[39m get_validation_metrics(val_loader, model, loss_func, device)\n\u001b[1;32m    206\u001b[0m lr_scheduler\u001b[38;5;241m.\u001b[39mstep(v_loss)\n\u001b[1;32m    208\u001b[0m row \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    209\u001b[0m     tl, v_loss,\n\u001b[1;32m    210\u001b[0m     ta,  v_acc,\n\u001b[1;32m    211\u001b[0m     tm,  v_mse,\n\u001b[1;32m    212\u001b[0m     tma, v_mae,\n\u001b[1;32m    213\u001b[0m ]\n",
      "File \u001b[0;32m~/gat-cd4c/src/train.py:246\u001b[0m, in \u001b[0;36mget_validation_metrics\u001b[0;34m(loader, model, loss_func, device)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[1;32m    245\u001b[0m     d_n, d_e, d_a, p_n, p_e, p_a, labels \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mto(device, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m batch]\n\u001b[0;32m--> 246\u001b[0m     preds \u001b[38;5;241m=\u001b[39m model(d_n, d_e, d_a, p_n, p_e, p_a)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    247\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_func(preds, labels)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    248\u001b[0m     acc \u001b[38;5;241m=\u001b[39m accuracy_func(preds, labels, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n",
      "File \u001b[0;32m/scratch/local3/2024/aguo/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/scratch/local3/2024/aguo/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/gat-cd4c/src/model.py:147\u001b[0m, in \u001b[0;36mDualGraphAttentionNetwork.forward\u001b[0;34m(self, drug_node_feats, drug_edge_feats, drug_adj, prot_node_feats, prot_edge_feats, prot_adj)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    137\u001b[0m             drug_node_feats: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    138\u001b[0m             drug_edge_feats: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    142\u001b[0m             prot_adj: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;66;03m# Encode each graph\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     drug_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrug_encoder(drug_node_feats,\n\u001b[1;32m    145\u001b[0m                                  drug_edge_feats,\n\u001b[1;32m    146\u001b[0m                                  drug_adj)           \u001b[38;5;66;03m# [B]\u001b[39;00m\n\u001b[0;32m--> 147\u001b[0m     prot_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprot_encoder(prot_node_feats,\n\u001b[1;32m    148\u001b[0m                                  prot_edge_feats,\n\u001b[1;32m    149\u001b[0m                                  prot_adj)           \u001b[38;5;66;03m# [B]\u001b[39;00m\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;66;03m# Concatenate and project\u001b[39;00m\n\u001b[1;32m    151\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([drug_emb, prot_emb], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)   \u001b[38;5;66;03m# [B, 2]\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/local3/2024/aguo/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/scratch/local3/2024/aguo/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/gat-cd4c/src/model.py:95\u001b[0m, in \u001b[0;36mGraphAttentionEncoder.forward\u001b[0;34m(self, node_feats, edge_feats, adj)\u001b[0m\n\u001b[1;32m     93\u001b[0m x, e, a \u001b[38;5;241m=\u001b[39m node_feats, edge_feats, adj\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m gat \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgat_layers:\n\u001b[0;32m---> 95\u001b[0m     x, e, a \u001b[38;5;241m=\u001b[39m gat((x, e, a))\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# x: [B, N, out_features]\u001b[39;00m\n\u001b[1;32m     97\u001b[0m graph_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglobal_pool(x)     \u001b[38;5;66;03m# [B, 1]\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/local3/2024/aguo/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/scratch/local3/2024/aguo/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/gat-cd4c/src/model.py:34\u001b[0m, in \u001b[0;36mGPSLayer.forward\u001b[0;34m(self, x_e_a)\u001b[0m\n\u001b[1;32m     31\u001b[0m residual \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# local message-passing\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m local_x, edge, adj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocal((x, edge, adj))  \u001b[38;5;66;03m# [B, N, F]\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# global self-attention directly on (B, N, F)\u001b[39;00m\n\u001b[1;32m     36\u001b[0m attn_out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglobal_attn(local_x, local_x, local_x)\n",
      "File \u001b[0;32m/scratch/local3/2024/aguo/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/scratch/local3/2024/aguo/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/gat-cd4c/src/model.py:361\u001b[0m, in \u001b[0;36mGraphAttentionLayer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    358\u001b[0m attn_coeffs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_attn_coeffs(new_node_features, new_edge_features, adjacency_matrix, num_nodes)\n\u001b[1;32m    360\u001b[0m \u001b[38;5;66;03m# [B, N, num_heads, F_out // num_attn_heads] -> [B, N, F_out]\u001b[39;00m\n\u001b[0;32m--> 361\u001b[0m new_node_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute_message_passing(new_node_features, attn_coeffs, batch_size, num_nodes)\n\u001b[1;32m    363\u001b[0m \u001b[38;5;66;03m# Do final projection and dropout\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;66;03m# The shape remains [B, N, F_out]\u001b[39;00m\n\u001b[1;32m    365\u001b[0m new_node_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_node_projection(new_node_features)\n",
      "File \u001b[0;32m~/gat-cd4c/src/model.py:427\u001b[0m, in \u001b[0;36mGraphAttentionLayer._execute_message_passing\u001b[0;34m(self, node_features, attn_coeffs, batch_size, num_nodes)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Perform message passing based on computed attention coefficients.\"\"\"\u001b[39;00m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;66;03m# [B, N, num_heads, F_out // num_heads] EINSUM [B, N, N, num_heads]\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;66;03m# -> [B, N, num_heads, F_out // num_heads]\u001b[39;00m\n\u001b[0;32m--> 427\u001b[0m new_node_features \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbmax, bnma -> bnax\u001b[39m\u001b[38;5;124m'\u001b[39m, node_features, attn_coeffs)\n\u001b[1;32m    429\u001b[0m \u001b[38;5;66;03m# Concatenate output for different attention heads together.\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;66;03m# [B, N, num_heads, F_out // num_heads] -> [B, N, F_out]\u001b[39;00m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_node_features\u001b[38;5;241m.\u001b[39mview(batch_size, num_nodes, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/scratch/local3/2024/aguo/miniconda3/lib/python3.12/site-packages/torch/functional.py:402\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m einsum(equation, \u001b[38;5;241m*\u001b[39m_operands)\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(operands) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39menabled:\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;66;03m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;66;03m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[0;32m--> 402\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39meinsum(equation, operands)  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    404\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39mis_available():\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "training_args = argparse.Namespace(**args)\n",
    "train_model(training_args, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
